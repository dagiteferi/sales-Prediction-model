{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "## 1.1 Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (model.py, line 113)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Dagi\\Documents\\KAIM\\Week-4\\sales-Prediction-model\\week-4\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 26\u001b[1;36m\n\u001b[1;33m    from scripts.model import ModelTrainer\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\Dagi\\Documents\\KAIM\\Week-4\\sales-Prediction-model\\scripts\\model.py:113\u001b[1;36m\u001b[0m\n\u001b[1;33m    time steps = 60\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Add the root of the project to the Python path\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    from scripts.preprocess import Preprocessor\n",
    "    from scripts.model import ModelTrainer\n",
    "    print(\"Custom module imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Warnings and Logging Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings and logging\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Append the correct path for custom module imports\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "sys.path.append(os.path.abspath('../notebook/models'))\n",
    "\n",
    "# Optional custom logging (currently commented out)\n",
    "# from custom_logging import info_logger, error_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Commented Out Imports for Future Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_processing import load_data, clean_data, handle_missing_values\n",
    "# from sale_analysis import (\n",
    "#     plot_sales_distribution,\n",
    "#     compare_sales_holidays,\n",
    "#     seasonal_behavior,\n",
    "#     correlation_analysis,\n",
    "#     promo_effect,\n",
    "#     effective_promo_deployment,\n",
    "#     customer_behavior_trends,\n",
    "#     weekday_openings,\n",
    "#     assortment_type_impact,\n",
    "#     competitor_distance_impact,\n",
    "#     new_competitor_effects, \n",
    "#     plot_promo_distribution,\n",
    "#     plot_sales_during_holidays,\n",
    "#     plot_sales_customers_corr, \n",
    "#     plot_store_corr\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the datasets\n",
    "train_path = '../data/train.csv'  # Update to the correct path on your local machine\n",
    "test_path = '../data/test.csv'    # Update to the correct path on your local machine\n",
    "store_path = '../data/store.csv'  # Update to the correct path on your local machine\n",
    "\n",
    "# Initialize the Preprocessor class\n",
    "preprocessor = Preprocessor(train_path, test_path, store_path)\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    train_data, test_data, store_data = preprocessor.load_data()\n",
    "    print(\"Datasets loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## 2.1 Summary of Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store             int64\n",
      "DayOfWeek         int64\n",
      "Date             object\n",
      "Sales             int64\n",
      "Customers         int64\n",
      "Open              int64\n",
      "Promo             int64\n",
      "StateHoliday     object\n",
      "SchoolHoliday     int64\n",
      "dtype: object\n",
      "              Store     DayOfWeek         Sales     Customers          Open  \\\n",
      "count  1.017209e+06  1.017209e+06  1.017209e+06  1.017209e+06  1.017209e+06   \n",
      "mean   5.584297e+02  3.998341e+00  5.773819e+03  6.331459e+02  8.301067e-01   \n",
      "std    3.219087e+02  1.997391e+00  3.849926e+03  4.644117e+02  3.755392e-01   \n",
      "min    1.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    2.800000e+02  2.000000e+00  3.727000e+03  4.050000e+02  1.000000e+00   \n",
      "50%    5.580000e+02  4.000000e+00  5.744000e+03  6.090000e+02  1.000000e+00   \n",
      "75%    8.380000e+02  6.000000e+00  7.856000e+03  8.370000e+02  1.000000e+00   \n",
      "max    1.115000e+03  7.000000e+00  4.155100e+04  7.388000e+03  1.000000e+00   \n",
      "\n",
      "              Promo  SchoolHoliday  \n",
      "count  1.017209e+06   1.017209e+06  \n",
      "mean   3.815145e-01   1.786467e-01  \n",
      "std    4.857586e-01   3.830564e-01  \n",
      "min    0.000000e+00   0.000000e+00  \n",
      "25%    0.000000e+00   0.000000e+00  \n",
      "50%    0.000000e+00   0.000000e+00  \n",
      "75%    1.000000e+00   0.000000e+00  \n",
      "max    1.000000e+00   1.000000e+00  \n",
      "Id                 int64\n",
      "Store              int64\n",
      "DayOfWeek          int64\n",
      "Date              object\n",
      "Open             float64\n",
      "Promo              int64\n",
      "StateHoliday      object\n",
      "SchoolHoliday      int64\n",
      "dtype: object\n",
      "Store                          int64\n",
      "StoreType                     object\n",
      "Assortment                    object\n",
      "CompetitionDistance          float64\n",
      "CompetitionOpenSinceMonth    float64\n",
      "CompetitionOpenSinceYear     float64\n",
      "Promo2                         int64\n",
      "Promo2SinceWeek              float64\n",
      "Promo2SinceYear              float64\n",
      "PromoInterval                 object\n",
      "dtype: object\n",
      "            Store  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
      "count  1115.00000          1112.000000                 761.000000   \n",
      "mean    558.00000          5404.901079                   7.224704   \n",
      "std     322.01708          7663.174720                   3.212348   \n",
      "min       1.00000            20.000000                   1.000000   \n",
      "25%     279.50000           717.500000                   4.000000   \n",
      "50%     558.00000          2325.000000                   8.000000   \n",
      "75%     836.50000          6882.500000                  10.000000   \n",
      "max    1115.00000         75860.000000                  12.000000   \n",
      "\n",
      "       CompetitionOpenSinceYear       Promo2  Promo2SinceWeek  Promo2SinceYear  \n",
      "count                761.000000  1115.000000       571.000000       571.000000  \n",
      "mean                2008.668857     0.512108        23.595447      2011.763573  \n",
      "std                    6.195983     0.500078        14.141984         1.674935  \n",
      "min                 1900.000000     0.000000         1.000000      2009.000000  \n",
      "25%                 2006.000000     0.000000        13.000000      2011.000000  \n",
      "50%                 2010.000000     1.000000        22.000000      2012.000000  \n",
      "75%                 2013.000000     1.000000        37.000000      2013.000000  \n",
      "max                 2015.000000     1.000000        50.000000      2015.000000  \n",
      "Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Sales            0\n",
      "Customers        0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64\n",
      "Id                0\n",
      "Store             0\n",
      "DayOfWeek         0\n",
      "Date              0\n",
      "Open             11\n",
      "Promo             0\n",
      "StateHoliday      0\n",
      "SchoolHoliday     0\n",
      "dtype: int64\n",
      "Store                          0\n",
      "StoreType                      0\n",
      "Assortment                     0\n",
      "CompetitionDistance            3\n",
      "CompetitionOpenSinceMonth    354\n",
      "CompetitionOpenSinceYear     354\n",
      "Promo2                         0\n",
      "Promo2SinceWeek              544\n",
      "Promo2SinceYear              544\n",
      "PromoInterval                544\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Summarize the dataset\n",
    "preprocessor.summarize_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Descriptive Statistics for Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.017209e+06</td>\n",
       "      <td>1.017209e+06</td>\n",
       "      <td>1.017209e+06</td>\n",
       "      <td>1.017209e+06</td>\n",
       "      <td>1.017209e+06</td>\n",
       "      <td>1.017209e+06</td>\n",
       "      <td>1.017209e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.584297e+02</td>\n",
       "      <td>3.998341e+00</td>\n",
       "      <td>5.773819e+03</td>\n",
       "      <td>6.331459e+02</td>\n",
       "      <td>8.301067e-01</td>\n",
       "      <td>3.815145e-01</td>\n",
       "      <td>1.786467e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.219087e+02</td>\n",
       "      <td>1.997391e+00</td>\n",
       "      <td>3.849926e+03</td>\n",
       "      <td>4.644117e+02</td>\n",
       "      <td>3.755392e-01</td>\n",
       "      <td>4.857586e-01</td>\n",
       "      <td>3.830564e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.800000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.727000e+03</td>\n",
       "      <td>4.050000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.580000e+02</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>5.744000e+03</td>\n",
       "      <td>6.090000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.380000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>7.856000e+03</td>\n",
       "      <td>8.370000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.115000e+03</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>4.155100e+04</td>\n",
       "      <td>7.388000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Store     DayOfWeek         Sales     Customers          Open  \\\n",
       "count  1.017209e+06  1.017209e+06  1.017209e+06  1.017209e+06  1.017209e+06   \n",
       "mean   5.584297e+02  3.998341e+00  5.773819e+03  6.331459e+02  8.301067e-01   \n",
       "std    3.219087e+02  1.997391e+00  3.849926e+03  4.644117e+02  3.755392e-01   \n",
       "min    1.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    2.800000e+02  2.000000e+00  3.727000e+03  4.050000e+02  1.000000e+00   \n",
       "50%    5.580000e+02  4.000000e+00  5.744000e+03  6.090000e+02  1.000000e+00   \n",
       "75%    8.380000e+02  6.000000e+00  7.856000e+03  8.370000e+02  1.000000e+00   \n",
       "max    1.115000e+03  7.000000e+00  4.155100e+04  7.388000e+03  1.000000e+00   \n",
       "\n",
       "              Promo  SchoolHoliday  \n",
       "count  1.017209e+06   1.017209e+06  \n",
       "mean   3.815145e-01   1.786467e-01  \n",
       "std    4.857586e-01   3.830564e-01  \n",
       "min    0.000000e+00   0.000000e+00  \n",
       "25%    0.000000e+00   0.000000e+00  \n",
       "50%    0.000000e+00   0.000000e+00  \n",
       "75%    1.000000e+00   0.000000e+00  \n",
       "max    1.000000e+00   1.000000e+00  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display descriptive statistics for the training dataset\n",
    "train_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Types and Descriptive Statistics for Test and Store Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                 int64\n",
      "Store              int64\n",
      "DayOfWeek          int64\n",
      "Date              object\n",
      "Open             float64\n",
      "Promo              int64\n",
      "StateHoliday      object\n",
      "SchoolHoliday      int64\n",
      "dtype: object\n",
      "Store                          int64\n",
      "StoreType                     object\n",
      "Assortment                    object\n",
      "CompetitionDistance          float64\n",
      "CompetitionOpenSinceMonth    float64\n",
      "CompetitionOpenSinceYear     float64\n",
      "Promo2                         int64\n",
      "Promo2SinceWeek              float64\n",
      "Promo2SinceYear              float64\n",
      "PromoInterval                 object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1115.00000</td>\n",
       "      <td>1112.000000</td>\n",
       "      <td>761.000000</td>\n",
       "      <td>761.000000</td>\n",
       "      <td>1115.000000</td>\n",
       "      <td>571.000000</td>\n",
       "      <td>571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>558.00000</td>\n",
       "      <td>5404.901079</td>\n",
       "      <td>7.224704</td>\n",
       "      <td>2008.668857</td>\n",
       "      <td>0.512108</td>\n",
       "      <td>23.595447</td>\n",
       "      <td>2011.763573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>322.01708</td>\n",
       "      <td>7663.174720</td>\n",
       "      <td>3.212348</td>\n",
       "      <td>6.195983</td>\n",
       "      <td>0.500078</td>\n",
       "      <td>14.141984</td>\n",
       "      <td>1.674935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>279.50000</td>\n",
       "      <td>717.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>558.00000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>836.50000</td>\n",
       "      <td>6882.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1115.00000</td>\n",
       "      <td>75860.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Store  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       "count  1115.00000          1112.000000                 761.000000   \n",
       "mean    558.00000          5404.901079                   7.224704   \n",
       "std     322.01708          7663.174720                   3.212348   \n",
       "min       1.00000            20.000000                   1.000000   \n",
       "25%     279.50000           717.500000                   4.000000   \n",
       "50%     558.00000          2325.000000                   8.000000   \n",
       "75%     836.50000          6882.500000                  10.000000   \n",
       "max    1115.00000         75860.000000                  12.000000   \n",
       "\n",
       "       CompetitionOpenSinceYear       Promo2  Promo2SinceWeek  Promo2SinceYear  \n",
       "count                761.000000  1115.000000       571.000000       571.000000  \n",
       "mean                2008.668857     0.512108        23.595447      2011.763573  \n",
       "std                    6.195983     0.500078        14.141984         1.674935  \n",
       "min                 1900.000000     0.000000         1.000000      2009.000000  \n",
       "25%                 2006.000000     0.000000        13.000000      2011.000000  \n",
       "50%                 2010.000000     1.000000        22.000000      2012.000000  \n",
       "75%                 2013.000000     1.000000        37.000000      2013.000000  \n",
       "max                 2015.000000     1.000000        50.000000      2015.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data types for the test dataset\n",
    "print(test_data.dtypes)\n",
    "\n",
    "# Display data types for the store dataset\n",
    "print(store_data.dtypes)\n",
    "\n",
    "# Display descriptive statistics for the store dataset\n",
    "store_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data Cleaning\n",
    "\n",
    "### 2.4.1 Checking Missing Values in Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store            0\n",
       "DayOfWeek        0\n",
       "Date             0\n",
       "Sales            0\n",
       "Customers        0\n",
       "Open             0\n",
       "Promo            0\n",
       "StateHoliday     0\n",
       "SchoolHoliday    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the training dataset\n",
    "train_data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Checking Missing Values in Testing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                0\n",
       "Store             0\n",
       "DayOfWeek         0\n",
       "Date              0\n",
       "Open             11\n",
       "Promo             0\n",
       "StateHoliday      0\n",
       "SchoolHoliday     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the testing dataset\n",
    "test_data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Checking Missing Values in Store Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                          0\n",
       "StoreType                      0\n",
       "Assortment                     0\n",
       "CompetitionDistance            3\n",
       "CompetitionOpenSinceMonth    354\n",
       "CompetitionOpenSinceYear     354\n",
       "Promo2                         0\n",
       "Promo2SinceWeek              544\n",
       "Promo2SinceYear              544\n",
       "PromoInterval                544\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the store dataset\n",
    "store_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Merge the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>PromoInterval</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>570.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Jan,Apr,Jul,Oct</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>14130.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Jan,Apr,Jul,Oct</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>13995.0</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>620.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>4822.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>29910.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date    Sales  Customers  Open  Promo StateHoliday  \\\n",
       "0      1          5  2015-07-31   5263.0      555.0   1.0      1            0   \n",
       "1      2          5  2015-07-31   6064.0      625.0   1.0      1            0   \n",
       "2      3          5  2015-07-31   8314.0      821.0   1.0      1            0   \n",
       "3      4          5  2015-07-31  13995.0     1498.0   1.0      1            0   \n",
       "4      5          5  2015-07-31   4822.0      559.0   1.0      1            0   \n",
       "\n",
       "   SchoolHoliday StoreType Assortment  CompetitionDistance  \\\n",
       "0              1         c          a               1270.0   \n",
       "1              1         a          a                570.0   \n",
       "2              1         a          a              14130.0   \n",
       "3              1         c          c                620.0   \n",
       "4              1         a          a              29910.0   \n",
       "\n",
       "   CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  \\\n",
       "0                        9.0                    2008.0       0   \n",
       "1                       11.0                    2007.0       1   \n",
       "2                       12.0                    2006.0       1   \n",
       "3                        9.0                    2009.0       0   \n",
       "4                        4.0                    2015.0       0   \n",
       "\n",
       "   Promo2SinceWeek  Promo2SinceYear    PromoInterval  Id  \n",
       "0              NaN              NaN              NaN NaN  \n",
       "1             13.0           2010.0  Jan,Apr,Jul,Oct NaN  \n",
       "2             14.0           2011.0  Jan,Apr,Jul,Oct NaN  \n",
       "3              NaN              NaN              NaN NaN  \n",
       "4              NaN              NaN              NaN NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the datasets using the Preprocessor class\n",
    "merged_data = preprocessor.merge_datasets()\n",
    "\n",
    "# Display the head of the final combined dataset\n",
    "merged_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1058297, 19)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the merged dataset\n",
    "merged_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Checking Missing Values in the Merged Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                              0\n",
       "DayOfWeek                          0\n",
       "Date                               0\n",
       "Sales                          41088\n",
       "Customers                      41088\n",
       "Open                              11\n",
       "Promo                              0\n",
       "StateHoliday                       0\n",
       "SchoolHoliday                      0\n",
       "StoreType                          0\n",
       "Assortment                         0\n",
       "CompetitionDistance             2738\n",
       "CompetitionOpenSinceMonth     338564\n",
       "CompetitionOpenSinceYear      338564\n",
       "Promo2                             0\n",
       "Promo2SinceWeek               525263\n",
       "Promo2SinceYear               525263\n",
       "PromoInterval                 525263\n",
       "Id                           1017209\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the merged dataset\n",
    "merged_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store                        0\n",
      "DayOfWeek                    0\n",
      "Date                         0\n",
      "Sales                        0\n",
      "Customers                    0\n",
      "Open                         0\n",
      "Promo                        0\n",
      "StateHoliday                 0\n",
      "SchoolHoliday                0\n",
      "StoreType                    0\n",
      "Assortment                   0\n",
      "CompetitionDistance          0\n",
      "CompetitionOpenSinceMonth    0\n",
      "CompetitionOpenSinceYear     0\n",
      "Promo2                       0\n",
      "Promo2SinceWeek              0\n",
      "Promo2SinceYear              0\n",
      "PromoInterval                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in the merged dataset\n",
    "cleaned_data = preprocessor.handle_missing_values()\n",
    "\n",
    "# Verify if there are still missing values\n",
    "print(cleaned_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Feature Engineering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime column to pandas datetime\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date-based features\n",
    "merged_data['Day'] = merged_data['Date'].dt.day\n",
    "merged_data['WeekOfYear'] = merged_data['Date'].dt.isocalendar().week\n",
    "merged_data['Month'] = merged_data['Date'].dt.month\n",
    "merged_data['Year'] = merged_data['Date'].dt.year\n",
    "merged_data['DayOfWeek'] = merged_data['Date'].dt.dayofweek\n",
    "merged_data['IsWeekend'] = merged_data['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "merged_data['IsBeginningOfMonth'] = merged_data['Day'].apply(lambda x: 1 if x <= 7 else 0)\n",
    "merged_data['IsMidMonth'] = merged_data['Day'].apply(lambda x: 1 if 8 <= x <= 21 else 0)\n",
    "merged_data['IsEndOfMonth'] = merged_data['Day'].apply(lambda x: 1 if x > 21 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  DayOfWeek       Date    Sales  Customers  Open  Promo StateHoliday  \\\n",
      "0      1          4 2015-07-31   5263.0      555.0   1.0      1            0   \n",
      "1      2          4 2015-07-31   6064.0      625.0   1.0      1            0   \n",
      "2      3          4 2015-07-31   8314.0      821.0   1.0      1            0   \n",
      "3      4          4 2015-07-31  13995.0     1498.0   1.0      1            0   \n",
      "4      5          4 2015-07-31   4822.0      559.0   1.0      1            0   \n",
      "\n",
      "   SchoolHoliday  StoreType  ...  Promo2SinceYear  PromoInterval  Day  \\\n",
      "0              1          2  ...              0.0              3   31   \n",
      "1              1          0  ...           2010.0              1   31   \n",
      "2              1          0  ...           2011.0              1   31   \n",
      "3              1          2  ...              0.0              3   31   \n",
      "4              1          0  ...              0.0              3   31   \n",
      "\n",
      "   WeekOfYear  Month  Year  IsWeekend  IsBeginningOfMonth  IsMidMonth  \\\n",
      "0          31      7  2015          0                   0           0   \n",
      "1          31      7  2015          0                   0           0   \n",
      "2          31      7  2015          0                   0           0   \n",
      "3          31      7  2015          0                   0           0   \n",
      "4          31      7  2015          0                   0           0   \n",
      "\n",
      "   IsEndOfMonth  \n",
      "0             1  \n",
      "1             1  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Additional features: Days to nearest holidays\n",
    "# Add logic here to compute 'days to next holiday' and 'days after holiday' based on a holiday dataset.\n",
    "\n",
    "# Label encode categorical features\n",
    "categorical_features = ['StoreType', 'Assortment', 'PromoInterval']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    merged_data[col] = label_encoder.fit_transform(merged_data[col].astype(str))\n",
    "\n",
    "# Handle missing values, e.g., fill missing competition distance with the median\n",
    "merged_data['CompetitionDistance'].fillna(merged_data['CompetitionDistance'].median(), inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_columns = ['CompetitionDistance', 'Day', 'WeekOfYear', 'Month', 'Year']\n",
    "merged_data[scaled_columns] = scaler.fit_transform(merged_data[scaled_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the target (sales) and features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the target (sales) and features\n",
    "X = merged_data.drop(columns=['Sales', 'Date'])\n",
    "y = merged_data['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check for non-numeric columns in the feature set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m non_numeric_columns \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-numeric columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnon_numeric_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Apply One-Hot Encoding to categorical columns (if any)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns in the feature set\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
    "\n",
    "# Apply One-Hot Encoding to categorical columns (if any)\n",
    "X_encoded = pd.get_dummies(X, columns=non_numeric_columns)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Training the XGBoost Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attributes after fitting:\n",
      "['_Booster', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__sklearn_is_fitted__', '__sklearn_tags__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_build_request_for_signature', '_can_use_inplace_predict', '_check_feature_names', '_check_n_features', '_configure_fit', '_create_dmatrix', '_doc_link_module', '_doc_link_template', '_doc_link_url_param_generator', '_estimator_type', '_get_default_requests', '_get_doc_link', '_get_iteration_range', '_get_metadata_request', '_get_param_names', '_get_tags', '_get_type', '_load_model_attributes', '_more_tags', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_set_evaluation_result', '_validate_data', '_validate_params', 'apply', 'base_score', 'best_iteration', 'best_score', 'booster', 'callbacks', 'coef_', 'colsample_bylevel', 'colsample_bynode', 'colsample_bytree', 'device', 'early_stopping_rounds', 'enable_categorical', 'eval_metric', 'evals_result', 'feature_importances_', 'feature_names_in_', 'feature_types', 'fit', 'gamma', 'get_booster', 'get_metadata_routing', 'get_num_boosting_rounds', 'get_params', 'get_xgb_params', 'grow_policy', 'importance_type', 'interaction_constraints', 'intercept_', 'learning_rate', 'load_model', 'max_bin', 'max_cat_threshold', 'max_cat_to_onehot', 'max_delta_step', 'max_depth', 'max_leaves', 'min_child_weight', 'missing', 'monotone_constraints', 'multi_strategy', 'n_estimators', 'n_features_in_', 'n_jobs', 'num_parallel_tree', 'objective', 'predict', 'random_state', 'reg_alpha', 'reg_lambda', 'sampling_method', 'save_model', 'scale_pos_weight', 'score', 'set_fit_request', 'set_params', 'set_predict_request', 'set_score_request', 'subsample', 'tree_method', 'validate_parameters', 'verbosity']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The XGBoost model is not fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Manually check if the model is fitted\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model_fitted(xgb_pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe XGBoost model is not fitted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m     30\u001b[0m y_pred_train_xgb \u001b[38;5;241m=\u001b[39m xgb_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The XGBoost model is not fitted."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Manually check if the model is fitted\n",
    "def is_model_fitted(model):\n",
    "    return hasattr(model, 'booster_')\n",
    "\n",
    "# Define XGBoost model within a pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', XGBRegressor(n_estimators=10, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Log the model's attributes to see if it has been fitted\n",
    "print(\"Model attributes after fitting:\")\n",
    "print(dir(xgb_pipeline.named_steps['model']))\n",
    "\n",
    "# Manually check if the model is fitted\n",
    "if not is_model_fitted(xgb_pipeline.named_steps['model']):\n",
    "    raise RuntimeError(\"The XGBoost model is not fitted.\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_train_xgb = xgb_pipeline.predict(X_train)\n",
    "y_pred_test_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_mse = mean_squared_error(y_train, y_pred_train_xgb)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train_xgb)\n",
    "train_r2 = r2_score(y_train, y_pred_train_xgb)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred_test_xgb)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test_xgb)\n",
    "test_r2 = r2_score(y_test, y_pred_test_xgb)\n",
    "\n",
    "print(f\"Training Data - MSE: {train_mse}, MAE: {train_mae}, R2: {train_r2}\")\n",
    "print(f\"Testing Data - MSE: {test_mse}, MAE: {test_mae}, R2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the evaluation functionss\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 16\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m     17\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Define the XGBoost model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_train_xgb = xgb_model.predict(X_train_scaled)\n",
    "y_pred_test_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "mse_train_xgb, mae_train_xgb, r2_train_xgb = evaluate_model(y_train, y_pred_train_xgb)\n",
    "print(f\"XGBoost Training Set - MSE: {mse_train_xgb}, MAE: {mae_train_xgb}, R2: {r2_train_xgb}\")\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "mse_test_xgb, mae_test_xgb, r2_test_xgb = evaluate_model(y_test, y_pred_test_xgb)\n",
    "print(f\"XGBoost Test Set - MSE: {mse_test_xgb}, MAE: {mae_test_xgb}, R2: {r2_test_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13 Training the Random Forest Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m rf_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[0;32m      8\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m rf_pipeline\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Extract the RandomForest model from the pipeline\u001b[39;00m\n\u001b[0;32m     15\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m rf_pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define RandomForest model within a pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extract the RandomForest model from the pipeline\n",
    "rf_model = rf_pipeline.named_steps['model']\n",
    "\n",
    "# Get feature importances from the RandomForest model\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Assuming X_train was previously encoded\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Check the length of feature_names and feature importances\n",
    "print(f\"Length of feature names: {len(feature_names)}\")\n",
    "print(f\"Length of feature importances: {len(importances)}\")\n",
    "\n",
    "# Display the feature importances with corresponding feature names\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14 Choosing a Loss Function and Evaluating the Random Forest Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mse, mae, r2\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate on training set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dagi\\Documents\\KAIM\\Week-4\\sales-Prediction-model\\week-4\\Lib\\site-packages\\sklearn\\pipeline.py:786\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    785\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n\u001b[0;32m    789\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Dagi\\Documents\\KAIM\\Week-4\\sales-Prediction-model\\week-4\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:1065\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1063\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m-> 1065\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\Dagi\\Documents\\KAIM\\Week-4\\sales-Prediction-model\\week-4\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:633\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    632\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39m_support_missing_values(X):\n\u001b[0;32m    634\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Define evaluation metrics\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_train = pipeline.predict(X_train)\n",
    "y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate on training set\n",
    "mse_train, mae_train, r2_train = evaluate_model(y_train, y_pred_train)\n",
    "print(f\"Training Set - MSE: {mse_train}, MAE: {mae_train}, R2: {r2_train}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "mse_test, mae_test, r2_test = evaluate_model(y_test, y_pred_test)\n",
    "print(f\"Test Set - MSE: {mse_test}, MAE: {mae_test}, R2: {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract the RandomForest model from the pipeline\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mrf_pipeline\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get feature importances from the RandomForest model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m importances \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mfeature_importances_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the RandomForest model from the pipeline\n",
    "rf_model = pipeline.named_steps['model']\n",
    "\n",
    "# Get feature importances from the RandomForest model\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Retrieve feature names after encoding\n",
    "feature_names = X_encoded.columns\n",
    "\n",
    "# Check the length of feature_names and feature importances\n",
    "print(f\"Length of feature names: {len(feature_names)}\")\n",
    "print(f\"Length of feature importances: {len(importances)}\")\n",
    "\n",
    "# Display the feature importances with corresponding feature names\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plotting the top n important features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mbarh(\u001b[43mfeature_importance_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m][:top_n], feature_importance_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m][:top_n], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskyblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop Important Features from RandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_importance_df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_n = 10  # Set the number of top features to display\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance_df['Feature'][:top_n], feature_importance_df['Importance'][:top_n], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top Important Features from RandomForest')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the confidence interval based on the standard deviation of predictions\n",
    "y_pred_std = np.std([tree.predict(X_test) for tree in rf_model.estimators_], axis=0)\n",
    "confidence_interval = 1.96 * y_pred_std  # 95% confidence interval\n",
    "\n",
    "# Plotting the predictions with confidence intervals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(y_test.index, y_pred_test_rf, yerr=confidence_interval, fmt='o', ecolor='r', capthick=2, label=\"Confidence Interval\")\n",
    "plt.scatter(y_test.index, y_test, color='blue', label='Actual Values', alpha=0.5)\n",
    "plt.title(\"Predictions with 95% Confidence Interval\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.18 Saving the Model with Timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the models directory within the Kaggle environment\n",
    "models_dir = '../notebook/models'  # Use relative path for Kaggle\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Serialize model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_filename = f\"{models_dir}/sales_model_{timestamp}.pkl\"\n",
    "joblib.dump(pipeline, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Correct the path\n",
    "sys.path.append(os.path.abspath('../notebook/models'))\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('../notebook/models', exist_ok=True)\n",
    "\n",
    "# Serialize model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_filename = f\"../notebook/models/sales_model_{timestamp}.pkl\"\n",
    "joblib.dump(pipeline, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time series data\n",
    "time_steps = 60\n",
    "\n",
    "def create_lagged_data(data, time_steps=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sales = df['Sales'].values\n",
    "scaled_sales = (sales - sales.mean()) / sales.std()  # Scale to (-1, 1)\n",
    "\n",
    "# Create supervised learning data for LSTM\n",
    "X_lstm, y_lstm = create_lagged_data(scaled_sales, time_steps)\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),\n",
    "    LSTM(50),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
    "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=64, validation_data=(X_test_lstm, y_test_lstm))\n",
    "\n",
    "# Predict with LSTM\n",
    "y_pred_lstm = model.predict(X_test_lstm)\n",
    "mse_lstm = mean_squared_error(y_test_lstm, y_pred_lstm)\n",
    "print(f\"LSTM Model - Test Set MSE: {mse_lstm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.21 Calculating Predictions and Metrics for the LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions\n",
    "y_pred_lstm = model.predict(X_test_lstm)\n",
    "\n",
    "# Rescale predictions back to original values\n",
    "y_pred_lstm_rescaled = y_pred_lstm * sales.std() + sales.mean()\n",
    "y_test_lstm_rescaled = y_test_lstm * sales.std() + sales.mean()\n",
    "\n",
    "# Calculate metrics\n",
    "mae_lstm = mean_absolute_error(y_test_lstm_rescaled, y_pred_lstm_rescaled)\n",
    "mse_lstm = mean_squared_error(y_test_lstm_rescaled, y_pred_lstm_rescaled)\n",
    "r2_lstm = r2_score(y_test_lstm_rescaled, y_pred_lstm_rescaled)\n",
    "\n",
    "print(f\"LSTM Model - Test Set MAE: {mae_lstm:.2f}\")\n",
    "print(f\"LSTM Model - Test Set MSE: {mse_lstm:.2f}\")\n",
    "print(f\"LSTM Model - Test Set R²: {r2_lstm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.22 Plotting LSTM Model Predictions vs Actual Sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_lstm_rescaled, label='Actual Sales', color='blue')\n",
    "plt.plot(y_pred_lstm_rescaled, label='Predicted Sales', color='orange')\n",
    "plt.title('LSTM Model Predictions vs Actual Sales')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a filename with a timestamp\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "lstm_model_filename = f\"../notebook/models/lstm_model_{timestamp}.h5\"\n",
    "\n",
    "# Save the model\n",
    "model.save(lstm_model_filename)\n",
    "print(f'LSTM model serialized to {lstm_model_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scaling the sales data\n",
    "sales = df['Sales'].values\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_sales = scaler.fit_transform(sales.reshape(-1, 1))\n",
    "\n",
    "# Create lagged data for LSTM\n",
    "def create_lagged_data(data, time_steps=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_steps = 60\n",
    "X_lstm, y_lstm = create_lagged_data(scaled_sales, time_steps)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be 3D [samples, time steps, features]\n",
    "X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
    "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1))\n",
    "\n",
    "# Build enhanced LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(100, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),\n",
    "    Dropout(0.2),  # Add dropout for regularization\n",
    "    LSTM(100, return_sequences=True),  # Additional LSTM layer\n",
    "    Dropout(0.2),  # Another dropout\n",
    "    LSTM(50, return_sequences=False),  # Final LSTM layer\n",
    "    Dense(50, activation='relu'),  # Dense layer to learn more complex representations\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with mean squared error loss\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=64, validation_data=(X_test_lstm, y_test_lstm), \n",
    "                    callbacks=[early_stop], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.24 Predicting and Evaluating the Enhanced LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_lstm = model.predict(X_test_lstm)\n",
    "\n",
    "# Rescale predictions back to original scale\n",
    "y_pred_lstm_rescaled = scaler.inverse_transform(y_pred_lstm)\n",
    "y_test_lstm_rescaled = scaler.inverse_transform(y_test_lstm)\n",
    "\n",
    "# Evaluate model\n",
    "mae_lstm = mean_absolute_error(y_test_lstm_rescaled, y_pred_lstm_rescaled)\n",
    "mse_lstm = mean_squared_error(y_test_lstm_rescaled, y_pred_lstm_rescaled)\n",
    "r2_lstm = r2_score(y_test_lstm_rescaled, y_pred_lstm_rescaled)\n",
    "\n",
    "print(f\"LSTM Model - Test Set MAE: {mae_lstm:.2f}\")\n",
    "print(f\"LSTM Model - Test Set MSE: {mse_lstm:.2f}\")\n",
    "print(f\"LSTM Model - Test Set R²: {r2_lstm:.2f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_lstm_rescaled, label='Actual Sales', color='blue')\n",
    "plt.plot(y_pred_lstm_rescaled, label='Predicted Sales', color='orange')\n",
    "plt.title('Enhanced LSTM Model Predictions vs Actual Sales')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.25 Saving the Model within the Kaggle Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models directory within the Kaggle environment\n",
    "models_dir = './models'  # Use relative path for Kaggle\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Serialize model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_filename = f\"{models_dir}/sales_model_{timestamp}.pkl\"\n",
    "joblib.dump(pipeline, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
